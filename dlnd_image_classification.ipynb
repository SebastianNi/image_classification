{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1386e6e2cc0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(np.arange(10))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=((None,) + image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Convolution\n",
    "    conv_W = tf.Variable(tf.truncated_normal(shape=(conv_ksize[0] ,conv_ksize[1], x_tensor.shape.as_list()[3], conv_num_outputs)))\n",
    "    conv_b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv   = tf.nn.conv2d(x_tensor, conv_W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME') + conv_b\n",
    "\n",
    "    # Activation\n",
    "    conv = tf.nn.tanh(conv)\n",
    "    \n",
    "    # Pooling\n",
    "    return tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME') \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Flatten\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc_W = tf.Variable(tf.truncated_normal(shape=(x_tensor.shape.as_list()[1], num_outputs)))\n",
    "    fc_b = tf.Variable(tf.zeros(num_outputs))\n",
    "    fc   = tf.matmul(x_tensor, fc_W) + fc_b    \n",
    "    return tf.nn.tanh(fc)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Output layer\n",
    "    fc_W = tf.Variable(tf.truncated_normal(shape=(x_tensor.shape.as_list()[1], num_outputs)))\n",
    "    fc_b = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.matmul(x_tensor, fc_W) + fc_b\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_new = conv2d_maxpool(x, conv_num_outputs=20, conv_ksize=(3,3), conv_strides=(1,1), pool_ksize=(7,7), pool_strides=(5,5))\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x_new = flatten(x_new)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x_new = fully_conn(x_new, 400)\n",
    "    x_new = tf.nn.dropout(x_new, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits = output(x_new, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={ x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    train_acc = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss: {:>10.4f} Training Accuracy: {:.6f} Validation Accuracy: {:.6f}'.format(loss, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:    10.5891 Training Accuracy: 0.075000 Validation Accuracy: 0.190200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     8.6104 Training Accuracy: 0.175000 Validation Accuracy: 0.236200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     9.2905 Training Accuracy: 0.200000 Validation Accuracy: 0.265800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     9.0249 Training Accuracy: 0.250000 Validation Accuracy: 0.266200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     8.5518 Training Accuracy: 0.275000 Validation Accuracy: 0.285400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     8.9223 Training Accuracy: 0.250000 Validation Accuracy: 0.291000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     8.8588 Training Accuracy: 0.300000 Validation Accuracy: 0.301400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     9.0071 Training Accuracy: 0.275000 Validation Accuracy: 0.316800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     8.4276 Training Accuracy: 0.375000 Validation Accuracy: 0.315000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     9.0728 Training Accuracy: 0.300000 Validation Accuracy: 0.341200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     8.8777 Training Accuracy: 0.350000 Validation Accuracy: 0.333400\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:    10.3022 Training Accuracy: 0.275000 Validation Accuracy: 0.345400\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     9.1256 Training Accuracy: 0.275000 Validation Accuracy: 0.343200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     9.7253 Training Accuracy: 0.350000 Validation Accuracy: 0.345400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     9.3972 Training Accuracy: 0.475000 Validation Accuracy: 0.350000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     8.4805 Training Accuracy: 0.350000 Validation Accuracy: 0.348800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     8.5455 Training Accuracy: 0.350000 Validation Accuracy: 0.354400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     9.2918 Training Accuracy: 0.350000 Validation Accuracy: 0.359000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     8.8464 Training Accuracy: 0.425000 Validation Accuracy: 0.369800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     9.5918 Training Accuracy: 0.325000 Validation Accuracy: 0.376000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     6.8173 Training Accuracy: 0.525000 Validation Accuracy: 0.376600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     7.0716 Training Accuracy: 0.375000 Validation Accuracy: 0.370200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     7.8105 Training Accuracy: 0.375000 Validation Accuracy: 0.377200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     7.4448 Training Accuracy: 0.400000 Validation Accuracy: 0.380200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     6.9439 Training Accuracy: 0.450000 Validation Accuracy: 0.398400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     7.3437 Training Accuracy: 0.400000 Validation Accuracy: 0.392600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     6.4159 Training Accuracy: 0.475000 Validation Accuracy: 0.401800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     7.0151 Training Accuracy: 0.450000 Validation Accuracy: 0.398600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     6.9924 Training Accuracy: 0.525000 Validation Accuracy: 0.407200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     6.5434 Training Accuracy: 0.500000 Validation Accuracy: 0.409600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     6.6347 Training Accuracy: 0.475000 Validation Accuracy: 0.413000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     6.1230 Training Accuracy: 0.500000 Validation Accuracy: 0.411800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     5.8582 Training Accuracy: 0.500000 Validation Accuracy: 0.420400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     5.8747 Training Accuracy: 0.500000 Validation Accuracy: 0.412200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     6.0654 Training Accuracy: 0.425000 Validation Accuracy: 0.417600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     5.1022 Training Accuracy: 0.525000 Validation Accuracy: 0.419600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     5.6886 Training Accuracy: 0.450000 Validation Accuracy: 0.427600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     6.2395 Training Accuracy: 0.425000 Validation Accuracy: 0.426600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     6.1189 Training Accuracy: 0.450000 Validation Accuracy: 0.427600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     5.4261 Training Accuracy: 0.475000 Validation Accuracy: 0.434200\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     5.9100 Training Accuracy: 0.450000 Validation Accuracy: 0.431400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     5.7857 Training Accuracy: 0.450000 Validation Accuracy: 0.431000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     5.6279 Training Accuracy: 0.525000 Validation Accuracy: 0.435800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     5.4386 Training Accuracy: 0.475000 Validation Accuracy: 0.440400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     5.4282 Training Accuracy: 0.550000 Validation Accuracy: 0.442600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     4.8551 Training Accuracy: 0.525000 Validation Accuracy: 0.442600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     5.1119 Training Accuracy: 0.450000 Validation Accuracy: 0.441600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     5.0145 Training Accuracy: 0.575000 Validation Accuracy: 0.445000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     5.1016 Training Accuracy: 0.550000 Validation Accuracy: 0.447600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     4.8835 Training Accuracy: 0.500000 Validation Accuracy: 0.450600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     4.6174 Training Accuracy: 0.525000 Validation Accuracy: 0.448600\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     4.5789 Training Accuracy: 0.500000 Validation Accuracy: 0.451600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     4.4199 Training Accuracy: 0.475000 Validation Accuracy: 0.460600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     4.4151 Training Accuracy: 0.500000 Validation Accuracy: 0.452000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     4.5165 Training Accuracy: 0.500000 Validation Accuracy: 0.455600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     4.0730 Training Accuracy: 0.525000 Validation Accuracy: 0.453800\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     4.0669 Training Accuracy: 0.500000 Validation Accuracy: 0.456400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     4.0619 Training Accuracy: 0.500000 Validation Accuracy: 0.459400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     3.3279 Training Accuracy: 0.525000 Validation Accuracy: 0.465000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     3.2620 Training Accuracy: 0.525000 Validation Accuracy: 0.458600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     3.4992 Training Accuracy: 0.500000 Validation Accuracy: 0.466800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     3.3358 Training Accuracy: 0.600000 Validation Accuracy: 0.465000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     3.3199 Training Accuracy: 0.550000 Validation Accuracy: 0.464800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     2.8725 Training Accuracy: 0.575000 Validation Accuracy: 0.468800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     3.0253 Training Accuracy: 0.525000 Validation Accuracy: 0.470800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     2.8527 Training Accuracy: 0.525000 Validation Accuracy: 0.470400\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     2.6524 Training Accuracy: 0.525000 Validation Accuracy: 0.470800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     2.6833 Training Accuracy: 0.500000 Validation Accuracy: 0.476000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     2.6051 Training Accuracy: 0.525000 Validation Accuracy: 0.474200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     2.0407 Training Accuracy: 0.575000 Validation Accuracy: 0.474600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     2.3489 Training Accuracy: 0.500000 Validation Accuracy: 0.479000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     2.2818 Training Accuracy: 0.625000 Validation Accuracy: 0.476800\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     2.3244 Training Accuracy: 0.550000 Validation Accuracy: 0.469800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     2.1555 Training Accuracy: 0.550000 Validation Accuracy: 0.474800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     2.3573 Training Accuracy: 0.525000 Validation Accuracy: 0.478200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     2.0708 Training Accuracy: 0.525000 Validation Accuracy: 0.480600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.8421 Training Accuracy: 0.600000 Validation Accuracy: 0.480200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.8792 Training Accuracy: 0.550000 Validation Accuracy: 0.482200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     2.0727 Training Accuracy: 0.525000 Validation Accuracy: 0.482400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.9558 Training Accuracy: 0.475000 Validation Accuracy: 0.477800\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     2.0090 Training Accuracy: 0.525000 Validation Accuracy: 0.482400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.9044 Training Accuracy: 0.525000 Validation Accuracy: 0.483000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.7493 Training Accuracy: 0.550000 Validation Accuracy: 0.480400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.7360 Training Accuracy: 0.525000 Validation Accuracy: 0.483000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.9316 Training Accuracy: 0.550000 Validation Accuracy: 0.484600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.7610 Training Accuracy: 0.625000 Validation Accuracy: 0.483200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.6649 Training Accuracy: 0.575000 Validation Accuracy: 0.489600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.6823 Training Accuracy: 0.575000 Validation Accuracy: 0.485800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.5505 Training Accuracy: 0.575000 Validation Accuracy: 0.486400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.6597 Training Accuracy: 0.600000 Validation Accuracy: 0.484800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.6338 Training Accuracy: 0.575000 Validation Accuracy: 0.487200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.6894 Training Accuracy: 0.550000 Validation Accuracy: 0.486200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.6691 Training Accuracy: 0.575000 Validation Accuracy: 0.486800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.4943 Training Accuracy: 0.550000 Validation Accuracy: 0.490400\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.5011 Training Accuracy: 0.575000 Validation Accuracy: 0.488200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.4601 Training Accuracy: 0.625000 Validation Accuracy: 0.484600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.4320 Training Accuracy: 0.650000 Validation Accuracy: 0.487600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.4148 Training Accuracy: 0.550000 Validation Accuracy: 0.481400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.3936 Training Accuracy: 0.600000 Validation Accuracy: 0.487000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.3762 Training Accuracy: 0.625000 Validation Accuracy: 0.490200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     7.6839 Training Accuracy: 0.225000 Validation Accuracy: 0.195800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     9.3718 Training Accuracy: 0.225000 Validation Accuracy: 0.233400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     8.3875 Training Accuracy: 0.125000 Validation Accuracy: 0.250400\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     6.7181 Training Accuracy: 0.325000 Validation Accuracy: 0.270400\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     9.1179 Training Accuracy: 0.200000 Validation Accuracy: 0.277400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     7.4389 Training Accuracy: 0.375000 Validation Accuracy: 0.295400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     8.3594 Training Accuracy: 0.250000 Validation Accuracy: 0.299600\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     7.4633 Training Accuracy: 0.350000 Validation Accuracy: 0.309200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     5.8039 Training Accuracy: 0.300000 Validation Accuracy: 0.314800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     6.9725 Training Accuracy: 0.350000 Validation Accuracy: 0.306800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     6.6092 Training Accuracy: 0.375000 Validation Accuracy: 0.335600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     7.7867 Training Accuracy: 0.325000 Validation Accuracy: 0.337000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     5.5928 Training Accuracy: 0.425000 Validation Accuracy: 0.349400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     6.6718 Training Accuracy: 0.325000 Validation Accuracy: 0.350600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     6.8401 Training Accuracy: 0.375000 Validation Accuracy: 0.361800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     6.1338 Training Accuracy: 0.475000 Validation Accuracy: 0.364000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     6.5069 Training Accuracy: 0.325000 Validation Accuracy: 0.368600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     5.8760 Training Accuracy: 0.475000 Validation Accuracy: 0.382600\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     5.3650 Training Accuracy: 0.425000 Validation Accuracy: 0.376600\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     7.3328 Training Accuracy: 0.325000 Validation Accuracy: 0.379200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     6.2592 Training Accuracy: 0.400000 Validation Accuracy: 0.386000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     6.5946 Training Accuracy: 0.325000 Validation Accuracy: 0.389000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     5.1982 Training Accuracy: 0.500000 Validation Accuracy: 0.383200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     6.3756 Training Accuracy: 0.325000 Validation Accuracy: 0.398200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     6.8964 Training Accuracy: 0.375000 Validation Accuracy: 0.394400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     5.7810 Training Accuracy: 0.500000 Validation Accuracy: 0.402000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     5.4759 Training Accuracy: 0.425000 Validation Accuracy: 0.413600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     5.0021 Training Accuracy: 0.450000 Validation Accuracy: 0.405400\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     5.1353 Training Accuracy: 0.425000 Validation Accuracy: 0.410600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     5.5102 Training Accuracy: 0.425000 Validation Accuracy: 0.400800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     5.6349 Training Accuracy: 0.475000 Validation Accuracy: 0.415200\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     5.1209 Training Accuracy: 0.375000 Validation Accuracy: 0.419400\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     3.6751 Training Accuracy: 0.425000 Validation Accuracy: 0.419600\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     6.2972 Training Accuracy: 0.350000 Validation Accuracy: 0.417400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     6.8359 Training Accuracy: 0.375000 Validation Accuracy: 0.423800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     6.2639 Training Accuracy: 0.425000 Validation Accuracy: 0.433600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     4.7732 Training Accuracy: 0.375000 Validation Accuracy: 0.426800\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     4.1455 Training Accuracy: 0.500000 Validation Accuracy: 0.428800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     4.7758 Training Accuracy: 0.425000 Validation Accuracy: 0.436000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     5.4619 Training Accuracy: 0.400000 Validation Accuracy: 0.427800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     6.0270 Training Accuracy: 0.375000 Validation Accuracy: 0.436000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     5.4609 Training Accuracy: 0.350000 Validation Accuracy: 0.431200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     3.4040 Training Accuracy: 0.500000 Validation Accuracy: 0.426400\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     3.9310 Training Accuracy: 0.450000 Validation Accuracy: 0.433000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     5.3450 Training Accuracy: 0.400000 Validation Accuracy: 0.432000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     5.0927 Training Accuracy: 0.450000 Validation Accuracy: 0.448800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     3.9441 Training Accuracy: 0.450000 Validation Accuracy: 0.445200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     2.8236 Training Accuracy: 0.450000 Validation Accuracy: 0.448600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     3.1319 Training Accuracy: 0.525000 Validation Accuracy: 0.447200\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     4.8010 Training Accuracy: 0.425000 Validation Accuracy: 0.437200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     4.8058 Training Accuracy: 0.400000 Validation Accuracy: 0.454800\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     3.5132 Training Accuracy: 0.450000 Validation Accuracy: 0.445600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     2.4523 Training Accuracy: 0.550000 Validation Accuracy: 0.450000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     3.0715 Training Accuracy: 0.500000 Validation Accuracy: 0.445800\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     4.2246 Training Accuracy: 0.450000 Validation Accuracy: 0.447600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     4.7337 Training Accuracy: 0.425000 Validation Accuracy: 0.454800\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     3.1877 Training Accuracy: 0.500000 Validation Accuracy: 0.458000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     2.1424 Training Accuracy: 0.550000 Validation Accuracy: 0.457200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     2.9048 Training Accuracy: 0.475000 Validation Accuracy: 0.460200\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     4.0551 Training Accuracy: 0.425000 Validation Accuracy: 0.455400\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     4.0049 Training Accuracy: 0.425000 Validation Accuracy: 0.466800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     2.9760 Training Accuracy: 0.475000 Validation Accuracy: 0.466200\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.5332 Training Accuracy: 0.525000 Validation Accuracy: 0.470000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     2.2806 Training Accuracy: 0.475000 Validation Accuracy: 0.465000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     3.3339 Training Accuracy: 0.375000 Validation Accuracy: 0.467200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     3.2765 Training Accuracy: 0.525000 Validation Accuracy: 0.476200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     2.4623 Training Accuracy: 0.525000 Validation Accuracy: 0.471000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.3350 Training Accuracy: 0.550000 Validation Accuracy: 0.463000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.8943 Training Accuracy: 0.525000 Validation Accuracy: 0.464000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     2.7175 Training Accuracy: 0.450000 Validation Accuracy: 0.465800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.6931 Training Accuracy: 0.475000 Validation Accuracy: 0.464800\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     2.2136 Training Accuracy: 0.475000 Validation Accuracy: 0.461000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.0920 Training Accuracy: 0.600000 Validation Accuracy: 0.467200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.6347 Training Accuracy: 0.475000 Validation Accuracy: 0.469200\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     2.2305 Training Accuracy: 0.425000 Validation Accuracy: 0.472600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.4690 Training Accuracy: 0.450000 Validation Accuracy: 0.493400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     2.2749 Training Accuracy: 0.450000 Validation Accuracy: 0.477800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1933 Training Accuracy: 0.500000 Validation Accuracy: 0.471000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.6991 Training Accuracy: 0.450000 Validation Accuracy: 0.474800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     2.1772 Training Accuracy: 0.425000 Validation Accuracy: 0.466400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     2.3402 Training Accuracy: 0.500000 Validation Accuracy: 0.482200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.9782 Training Accuracy: 0.550000 Validation Accuracy: 0.480000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1216 Training Accuracy: 0.650000 Validation Accuracy: 0.468000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.6967 Training Accuracy: 0.475000 Validation Accuracy: 0.475000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     2.0654 Training Accuracy: 0.425000 Validation Accuracy: 0.477400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     2.1338 Training Accuracy: 0.475000 Validation Accuracy: 0.481600\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.8865 Training Accuracy: 0.475000 Validation Accuracy: 0.465600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.1487 Training Accuracy: 0.475000 Validation Accuracy: 0.469600\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.5506 Training Accuracy: 0.425000 Validation Accuracy: 0.474400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     2.1029 Training Accuracy: 0.425000 Validation Accuracy: 0.478800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.9619 Training Accuracy: 0.375000 Validation Accuracy: 0.477600\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.6417 Training Accuracy: 0.475000 Validation Accuracy: 0.472600\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.1392 Training Accuracy: 0.575000 Validation Accuracy: 0.477000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.4520 Training Accuracy: 0.525000 Validation Accuracy: 0.483400\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.7075 Training Accuracy: 0.400000 Validation Accuracy: 0.479600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8369 Training Accuracy: 0.400000 Validation Accuracy: 0.484400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.5459 Training Accuracy: 0.450000 Validation Accuracy: 0.477800\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.1749 Training Accuracy: 0.625000 Validation Accuracy: 0.471400\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.5003 Training Accuracy: 0.475000 Validation Accuracy: 0.478000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.7185 Training Accuracy: 0.450000 Validation Accuracy: 0.471400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.7124 Training Accuracy: 0.475000 Validation Accuracy: 0.486000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.5168 Training Accuracy: 0.475000 Validation Accuracy: 0.478600\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.1562 Training Accuracy: 0.650000 Validation Accuracy: 0.484800\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.4628 Training Accuracy: 0.500000 Validation Accuracy: 0.487200\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.6246 Training Accuracy: 0.475000 Validation Accuracy: 0.484800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.5836 Training Accuracy: 0.475000 Validation Accuracy: 0.491800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.4021 Training Accuracy: 0.500000 Validation Accuracy: 0.477000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.1770 Training Accuracy: 0.625000 Validation Accuracy: 0.480000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.4438 Training Accuracy: 0.575000 Validation Accuracy: 0.490200\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.5523 Training Accuracy: 0.475000 Validation Accuracy: 0.486200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.5475 Training Accuracy: 0.475000 Validation Accuracy: 0.495000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.4003 Training Accuracy: 0.450000 Validation Accuracy: 0.487000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.2068 Training Accuracy: 0.625000 Validation Accuracy: 0.479400\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.3759 Training Accuracy: 0.550000 Validation Accuracy: 0.492400\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.5808 Training Accuracy: 0.500000 Validation Accuracy: 0.489200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.5197 Training Accuracy: 0.450000 Validation Accuracy: 0.494400\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.3985 Training Accuracy: 0.500000 Validation Accuracy: 0.483000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.2389 Training Accuracy: 0.600000 Validation Accuracy: 0.486400\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.3607 Training Accuracy: 0.475000 Validation Accuracy: 0.498000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.5772 Training Accuracy: 0.475000 Validation Accuracy: 0.486200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.5253 Training Accuracy: 0.425000 Validation Accuracy: 0.499000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.3886 Training Accuracy: 0.500000 Validation Accuracy: 0.495400\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.2453 Training Accuracy: 0.525000 Validation Accuracy: 0.484000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.4052 Training Accuracy: 0.475000 Validation Accuracy: 0.497000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.5756 Training Accuracy: 0.475000 Validation Accuracy: 0.492800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.5291 Training Accuracy: 0.450000 Validation Accuracy: 0.500400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.3960 Training Accuracy: 0.475000 Validation Accuracy: 0.487000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.2329 Training Accuracy: 0.625000 Validation Accuracy: 0.479200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.3706 Training Accuracy: 0.550000 Validation Accuracy: 0.498000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.5306 Training Accuracy: 0.550000 Validation Accuracy: 0.497000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.5251 Training Accuracy: 0.450000 Validation Accuracy: 0.498400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.3635 Training Accuracy: 0.525000 Validation Accuracy: 0.491800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.2267 Training Accuracy: 0.625000 Validation Accuracy: 0.490000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.3947 Training Accuracy: 0.500000 Validation Accuracy: 0.499000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.5343 Training Accuracy: 0.500000 Validation Accuracy: 0.504800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.4975 Training Accuracy: 0.500000 Validation Accuracy: 0.505000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.3479 Training Accuracy: 0.525000 Validation Accuracy: 0.494200\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.1999 Training Accuracy: 0.650000 Validation Accuracy: 0.491000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.3347 Training Accuracy: 0.550000 Validation Accuracy: 0.495200\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.5708 Training Accuracy: 0.450000 Validation Accuracy: 0.502000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.4854 Training Accuracy: 0.475000 Validation Accuracy: 0.500800\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.3562 Training Accuracy: 0.550000 Validation Accuracy: 0.497200\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.1910 Training Accuracy: 0.625000 Validation Accuracy: 0.499000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.3364 Training Accuracy: 0.575000 Validation Accuracy: 0.505000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.5264 Training Accuracy: 0.475000 Validation Accuracy: 0.505800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.4992 Training Accuracy: 0.500000 Validation Accuracy: 0.504600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.3351 Training Accuracy: 0.500000 Validation Accuracy: 0.500400\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.2051 Training Accuracy: 0.600000 Validation Accuracy: 0.495200\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.3444 Training Accuracy: 0.550000 Validation Accuracy: 0.506600\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.5003 Training Accuracy: 0.475000 Validation Accuracy: 0.511400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.5140 Training Accuracy: 0.400000 Validation Accuracy: 0.512200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.3249 Training Accuracy: 0.525000 Validation Accuracy: 0.505000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.1613 Training Accuracy: 0.575000 Validation Accuracy: 0.500600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.3497 Training Accuracy: 0.500000 Validation Accuracy: 0.511200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.5012 Training Accuracy: 0.500000 Validation Accuracy: 0.510200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.4559 Training Accuracy: 0.450000 Validation Accuracy: 0.516600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.3142 Training Accuracy: 0.550000 Validation Accuracy: 0.505800\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     1.1357 Training Accuracy: 0.600000 Validation Accuracy: 0.504600\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.3687 Training Accuracy: 0.475000 Validation Accuracy: 0.512200\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.4984 Training Accuracy: 0.500000 Validation Accuracy: 0.513600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.4942 Training Accuracy: 0.400000 Validation Accuracy: 0.514200\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.3727 Training Accuracy: 0.500000 Validation Accuracy: 0.519400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     1.1226 Training Accuracy: 0.675000 Validation Accuracy: 0.507200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.3393 Training Accuracy: 0.550000 Validation Accuracy: 0.504600\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.4745 Training Accuracy: 0.475000 Validation Accuracy: 0.518800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.4552 Training Accuracy: 0.475000 Validation Accuracy: 0.520600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.3116 Training Accuracy: 0.525000 Validation Accuracy: 0.514000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     1.1275 Training Accuracy: 0.550000 Validation Accuracy: 0.506800\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.3229 Training Accuracy: 0.525000 Validation Accuracy: 0.514800\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.4702 Training Accuracy: 0.450000 Validation Accuracy: 0.514800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.3954 Training Accuracy: 0.475000 Validation Accuracy: 0.523800\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.2745 Training Accuracy: 0.525000 Validation Accuracy: 0.512000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     1.1169 Training Accuracy: 0.625000 Validation Accuracy: 0.508800\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.3339 Training Accuracy: 0.525000 Validation Accuracy: 0.517000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.4633 Training Accuracy: 0.500000 Validation Accuracy: 0.523200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.4413 Training Accuracy: 0.525000 Validation Accuracy: 0.525200\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.2459 Training Accuracy: 0.600000 Validation Accuracy: 0.524600\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.1139 Training Accuracy: 0.625000 Validation Accuracy: 0.516600\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.3048 Training Accuracy: 0.550000 Validation Accuracy: 0.524000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.4280 Training Accuracy: 0.475000 Validation Accuracy: 0.529600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.3975 Training Accuracy: 0.550000 Validation Accuracy: 0.522200\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.2347 Training Accuracy: 0.575000 Validation Accuracy: 0.522400\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     1.1021 Training Accuracy: 0.575000 Validation Accuracy: 0.519200\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.2952 Training Accuracy: 0.525000 Validation Accuracy: 0.525200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.4890 Training Accuracy: 0.475000 Validation Accuracy: 0.531800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.3393 Training Accuracy: 0.550000 Validation Accuracy: 0.528200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.1821 Training Accuracy: 0.550000 Validation Accuracy: 0.528800\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     1.0896 Training Accuracy: 0.600000 Validation Accuracy: 0.516600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.2955 Training Accuracy: 0.550000 Validation Accuracy: 0.525600\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.4209 Training Accuracy: 0.475000 Validation Accuracy: 0.531000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.3584 Training Accuracy: 0.525000 Validation Accuracy: 0.523000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.1576 Training Accuracy: 0.600000 Validation Accuracy: 0.523800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     1.0482 Training Accuracy: 0.700000 Validation Accuracy: 0.522400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.2815 Training Accuracy: 0.600000 Validation Accuracy: 0.527800\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.3909 Training Accuracy: 0.500000 Validation Accuracy: 0.533200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.3441 Training Accuracy: 0.575000 Validation Accuracy: 0.536000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     1.2552 Training Accuracy: 0.525000 Validation Accuracy: 0.533400\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     1.0904 Training Accuracy: 0.650000 Validation Accuracy: 0.526800\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.2649 Training Accuracy: 0.525000 Validation Accuracy: 0.537400\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.3688 Training Accuracy: 0.525000 Validation Accuracy: 0.538000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.3800 Training Accuracy: 0.525000 Validation Accuracy: 0.534200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     1.2149 Training Accuracy: 0.575000 Validation Accuracy: 0.536600\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     1.0707 Training Accuracy: 0.625000 Validation Accuracy: 0.529800\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.2760 Training Accuracy: 0.550000 Validation Accuracy: 0.532400\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.4105 Training Accuracy: 0.525000 Validation Accuracy: 0.542200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.3598 Training Accuracy: 0.575000 Validation Accuracy: 0.535200\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     1.1741 Training Accuracy: 0.525000 Validation Accuracy: 0.529200\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     1.0513 Training Accuracy: 0.600000 Validation Accuracy: 0.535200\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.3108 Training Accuracy: 0.525000 Validation Accuracy: 0.542800\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     1.3915 Training Accuracy: 0.425000 Validation Accuracy: 0.549200\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.3434 Training Accuracy: 0.550000 Validation Accuracy: 0.547400\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     1.1650 Training Accuracy: 0.600000 Validation Accuracy: 0.544200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     1.0200 Training Accuracy: 0.650000 Validation Accuracy: 0.539600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.2158 Training Accuracy: 0.625000 Validation Accuracy: 0.551400\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     1.3809 Training Accuracy: 0.475000 Validation Accuracy: 0.543400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.3544 Training Accuracy: 0.550000 Validation Accuracy: 0.546600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     1.1631 Training Accuracy: 0.575000 Validation Accuracy: 0.544600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     1.0072 Training Accuracy: 0.725000 Validation Accuracy: 0.541600\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.2484 Training Accuracy: 0.575000 Validation Accuracy: 0.545600\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     1.3697 Training Accuracy: 0.500000 Validation Accuracy: 0.550600\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.3502 Training Accuracy: 0.475000 Validation Accuracy: 0.544400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     1.1543 Training Accuracy: 0.600000 Validation Accuracy: 0.544200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.9814 Training Accuracy: 0.725000 Validation Accuracy: 0.542600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     1.2264 Training Accuracy: 0.625000 Validation Accuracy: 0.545200\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     1.3633 Training Accuracy: 0.475000 Validation Accuracy: 0.549400\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.3154 Training Accuracy: 0.550000 Validation Accuracy: 0.548800\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     1.1543 Training Accuracy: 0.600000 Validation Accuracy: 0.547200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.9804 Training Accuracy: 0.675000 Validation Accuracy: 0.546400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.2035 Training Accuracy: 0.600000 Validation Accuracy: 0.555600\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     1.3168 Training Accuracy: 0.550000 Validation Accuracy: 0.550000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.3214 Training Accuracy: 0.575000 Validation Accuracy: 0.549200\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     1.1300 Training Accuracy: 0.600000 Validation Accuracy: 0.546800\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     1.0107 Training Accuracy: 0.675000 Validation Accuracy: 0.541200\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     1.1745 Training Accuracy: 0.625000 Validation Accuracy: 0.551000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     1.3893 Training Accuracy: 0.500000 Validation Accuracy: 0.550000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.3457 Training Accuracy: 0.575000 Validation Accuracy: 0.553600\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     1.1377 Training Accuracy: 0.550000 Validation Accuracy: 0.546200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.9678 Training Accuracy: 0.700000 Validation Accuracy: 0.535600\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     1.2295 Training Accuracy: 0.575000 Validation Accuracy: 0.554400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     1.3688 Training Accuracy: 0.525000 Validation Accuracy: 0.561000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.3247 Training Accuracy: 0.575000 Validation Accuracy: 0.559600\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     1.1358 Training Accuracy: 0.575000 Validation Accuracy: 0.552800\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.9526 Training Accuracy: 0.650000 Validation Accuracy: 0.551400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.1756 Training Accuracy: 0.575000 Validation Accuracy: 0.556000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     1.3122 Training Accuracy: 0.525000 Validation Accuracy: 0.563200\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.3560 Training Accuracy: 0.525000 Validation Accuracy: 0.556800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     1.0813 Training Accuracy: 0.600000 Validation Accuracy: 0.553400\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.9589 Training Accuracy: 0.725000 Validation Accuracy: 0.546600\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.1971 Training Accuracy: 0.525000 Validation Accuracy: 0.552800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     1.2854 Training Accuracy: 0.525000 Validation Accuracy: 0.561200\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.3434 Training Accuracy: 0.575000 Validation Accuracy: 0.552200\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     1.1293 Training Accuracy: 0.550000 Validation Accuracy: 0.559200\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.9269 Training Accuracy: 0.700000 Validation Accuracy: 0.555400\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     1.1886 Training Accuracy: 0.600000 Validation Accuracy: 0.556200\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     1.3288 Training Accuracy: 0.500000 Validation Accuracy: 0.563200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.3143 Training Accuracy: 0.600000 Validation Accuracy: 0.559800\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     1.1049 Training Accuracy: 0.650000 Validation Accuracy: 0.551600\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.9002 Training Accuracy: 0.700000 Validation Accuracy: 0.551400\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.2149 Training Accuracy: 0.575000 Validation Accuracy: 0.559400\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     1.3258 Training Accuracy: 0.475000 Validation Accuracy: 0.558600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.3340 Training Accuracy: 0.550000 Validation Accuracy: 0.559000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     1.0551 Training Accuracy: 0.550000 Validation Accuracy: 0.559800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.8917 Training Accuracy: 0.700000 Validation Accuracy: 0.557600\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     1.1784 Training Accuracy: 0.550000 Validation Accuracy: 0.561200\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     1.3394 Training Accuracy: 0.500000 Validation Accuracy: 0.561200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.3061 Training Accuracy: 0.550000 Validation Accuracy: 0.560400\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     1.0880 Training Accuracy: 0.600000 Validation Accuracy: 0.562000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.9259 Training Accuracy: 0.675000 Validation Accuracy: 0.553000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     1.2196 Training Accuracy: 0.575000 Validation Accuracy: 0.563400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     1.3166 Training Accuracy: 0.500000 Validation Accuracy: 0.559800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.2732 Training Accuracy: 0.550000 Validation Accuracy: 0.559400\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     1.0999 Training Accuracy: 0.600000 Validation Accuracy: 0.564200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.9116 Training Accuracy: 0.675000 Validation Accuracy: 0.553200\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     1.1680 Training Accuracy: 0.650000 Validation Accuracy: 0.565400\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     1.2720 Training Accuracy: 0.550000 Validation Accuracy: 0.570000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2931 Training Accuracy: 0.550000 Validation Accuracy: 0.570000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     1.1286 Training Accuracy: 0.550000 Validation Accuracy: 0.555800\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.8966 Training Accuracy: 0.650000 Validation Accuracy: 0.557600\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     1.2032 Training Accuracy: 0.575000 Validation Accuracy: 0.566400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     1.2938 Training Accuracy: 0.475000 Validation Accuracy: 0.568200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.3074 Training Accuracy: 0.550000 Validation Accuracy: 0.570000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     1.0576 Training Accuracy: 0.625000 Validation Accuracy: 0.565400\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.9261 Training Accuracy: 0.675000 Validation Accuracy: 0.556000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     1.1754 Training Accuracy: 0.575000 Validation Accuracy: 0.569200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     1.2534 Training Accuracy: 0.550000 Validation Accuracy: 0.568400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.2279 Training Accuracy: 0.625000 Validation Accuracy: 0.569800\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     1.0819 Training Accuracy: 0.600000 Validation Accuracy: 0.572400\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.8840 Training Accuracy: 0.725000 Validation Accuracy: 0.568000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     1.1990 Training Accuracy: 0.600000 Validation Accuracy: 0.571600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     1.3135 Training Accuracy: 0.450000 Validation Accuracy: 0.563400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2826 Training Accuracy: 0.550000 Validation Accuracy: 0.564400\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     1.0821 Training Accuracy: 0.625000 Validation Accuracy: 0.566600\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.8533 Training Accuracy: 0.725000 Validation Accuracy: 0.561600\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     1.1914 Training Accuracy: 0.600000 Validation Accuracy: 0.570000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     1.2843 Training Accuracy: 0.525000 Validation Accuracy: 0.572400\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2652 Training Accuracy: 0.550000 Validation Accuracy: 0.569400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     1.0718 Training Accuracy: 0.550000 Validation Accuracy: 0.574200\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.8821 Training Accuracy: 0.750000 Validation Accuracy: 0.575400\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     1.1739 Training Accuracy: 0.575000 Validation Accuracy: 0.570200\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     1.3025 Training Accuracy: 0.450000 Validation Accuracy: 0.572800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.2483 Training Accuracy: 0.550000 Validation Accuracy: 0.569800\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     1.0191 Training Accuracy: 0.625000 Validation Accuracy: 0.565000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.8690 Training Accuracy: 0.725000 Validation Accuracy: 0.566800\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     1.1467 Training Accuracy: 0.575000 Validation Accuracy: 0.574800\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     1.2777 Training Accuracy: 0.525000 Validation Accuracy: 0.573000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2432 Training Accuracy: 0.575000 Validation Accuracy: 0.577600\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.9799 Training Accuracy: 0.600000 Validation Accuracy: 0.568600\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.8713 Training Accuracy: 0.700000 Validation Accuracy: 0.571400\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     1.1465 Training Accuracy: 0.575000 Validation Accuracy: 0.573400\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     1.2410 Training Accuracy: 0.525000 Validation Accuracy: 0.582800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.2314 Training Accuracy: 0.600000 Validation Accuracy: 0.573800\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     1.0719 Training Accuracy: 0.650000 Validation Accuracy: 0.572200\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.8342 Training Accuracy: 0.700000 Validation Accuracy: 0.567000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     1.1191 Training Accuracy: 0.600000 Validation Accuracy: 0.578600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     1.2461 Training Accuracy: 0.550000 Validation Accuracy: 0.581000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.2383 Training Accuracy: 0.575000 Validation Accuracy: 0.570400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.9890 Training Accuracy: 0.675000 Validation Accuracy: 0.573400\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.8418 Training Accuracy: 0.775000 Validation Accuracy: 0.574800\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     1.1257 Training Accuracy: 0.600000 Validation Accuracy: 0.576400\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     1.2239 Training Accuracy: 0.550000 Validation Accuracy: 0.583400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.2612 Training Accuracy: 0.600000 Validation Accuracy: 0.575400\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.9913 Training Accuracy: 0.600000 Validation Accuracy: 0.573600\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.8487 Training Accuracy: 0.775000 Validation Accuracy: 0.573600\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     1.1498 Training Accuracy: 0.575000 Validation Accuracy: 0.577800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     1.2329 Training Accuracy: 0.525000 Validation Accuracy: 0.581800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.2052 Training Accuracy: 0.600000 Validation Accuracy: 0.581400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     1.0101 Training Accuracy: 0.700000 Validation Accuracy: 0.580200\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.8231 Training Accuracy: 0.750000 Validation Accuracy: 0.571000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     1.1350 Training Accuracy: 0.600000 Validation Accuracy: 0.577200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     1.1737 Training Accuracy: 0.500000 Validation Accuracy: 0.579800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.2492 Training Accuracy: 0.550000 Validation Accuracy: 0.580800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     1.0013 Training Accuracy: 0.650000 Validation Accuracy: 0.578000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.8176 Training Accuracy: 0.750000 Validation Accuracy: 0.572200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     1.1709 Training Accuracy: 0.575000 Validation Accuracy: 0.581400\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     1.2260 Training Accuracy: 0.500000 Validation Accuracy: 0.578600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.2214 Training Accuracy: 0.600000 Validation Accuracy: 0.575200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.9768 Training Accuracy: 0.650000 Validation Accuracy: 0.582000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.8087 Training Accuracy: 0.800000 Validation Accuracy: 0.581400\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     1.1219 Training Accuracy: 0.550000 Validation Accuracy: 0.574800\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     1.2010 Training Accuracy: 0.550000 Validation Accuracy: 0.580800\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.2259 Training Accuracy: 0.600000 Validation Accuracy: 0.580600\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     1.0130 Training Accuracy: 0.600000 Validation Accuracy: 0.584600\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.8224 Training Accuracy: 0.775000 Validation Accuracy: 0.576800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     1.1499 Training Accuracy: 0.600000 Validation Accuracy: 0.584000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     1.1942 Training Accuracy: 0.575000 Validation Accuracy: 0.583000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.2484 Training Accuracy: 0.600000 Validation Accuracy: 0.583200\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.9850 Training Accuracy: 0.650000 Validation Accuracy: 0.581800\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.7966 Training Accuracy: 0.800000 Validation Accuracy: 0.578200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     1.1069 Training Accuracy: 0.625000 Validation Accuracy: 0.577600\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     1.2090 Training Accuracy: 0.600000 Validation Accuracy: 0.581000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.2477 Training Accuracy: 0.575000 Validation Accuracy: 0.584400\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.9561 Training Accuracy: 0.725000 Validation Accuracy: 0.581000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.8140 Training Accuracy: 0.725000 Validation Accuracy: 0.579000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     1.1065 Training Accuracy: 0.550000 Validation Accuracy: 0.589000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     1.2107 Training Accuracy: 0.525000 Validation Accuracy: 0.588200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1758 Training Accuracy: 0.625000 Validation Accuracy: 0.579800\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.9694 Training Accuracy: 0.675000 Validation Accuracy: 0.582200\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.8211 Training Accuracy: 0.725000 Validation Accuracy: 0.577800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     1.0794 Training Accuracy: 0.650000 Validation Accuracy: 0.590000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     1.1282 Training Accuracy: 0.600000 Validation Accuracy: 0.585200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.1990 Training Accuracy: 0.600000 Validation Accuracy: 0.588000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.9764 Training Accuracy: 0.675000 Validation Accuracy: 0.584400\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.8217 Training Accuracy: 0.800000 Validation Accuracy: 0.579400\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     1.0846 Training Accuracy: 0.625000 Validation Accuracy: 0.587600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     1.1844 Training Accuracy: 0.550000 Validation Accuracy: 0.588400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.2055 Training Accuracy: 0.600000 Validation Accuracy: 0.585600\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.9968 Training Accuracy: 0.650000 Validation Accuracy: 0.584200\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.7764 Training Accuracy: 0.850000 Validation Accuracy: 0.585200\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     1.1047 Training Accuracy: 0.550000 Validation Accuracy: 0.586800\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     1.1438 Training Accuracy: 0.600000 Validation Accuracy: 0.590800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.2759 Training Accuracy: 0.600000 Validation Accuracy: 0.580200\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.9504 Training Accuracy: 0.700000 Validation Accuracy: 0.582800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.7842 Training Accuracy: 0.750000 Validation Accuracy: 0.587400\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     1.0944 Training Accuracy: 0.575000 Validation Accuracy: 0.587000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     1.1360 Training Accuracy: 0.600000 Validation Accuracy: 0.589200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.1883 Training Accuracy: 0.650000 Validation Accuracy: 0.588200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.9982 Training Accuracy: 0.600000 Validation Accuracy: 0.591600\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.7849 Training Accuracy: 0.725000 Validation Accuracy: 0.583600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     1.0973 Training Accuracy: 0.575000 Validation Accuracy: 0.593400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     1.1643 Training Accuracy: 0.550000 Validation Accuracy: 0.593600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.1866 Training Accuracy: 0.625000 Validation Accuracy: 0.587200\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.9097 Training Accuracy: 0.675000 Validation Accuracy: 0.586600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.7771 Training Accuracy: 0.800000 Validation Accuracy: 0.585600\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     1.0877 Training Accuracy: 0.600000 Validation Accuracy: 0.592000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     1.1233 Training Accuracy: 0.575000 Validation Accuracy: 0.594600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.2065 Training Accuracy: 0.600000 Validation Accuracy: 0.591400\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.9433 Training Accuracy: 0.725000 Validation Accuracy: 0.590400\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.7950 Training Accuracy: 0.775000 Validation Accuracy: 0.584800\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     1.0849 Training Accuracy: 0.625000 Validation Accuracy: 0.590600\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     1.2004 Training Accuracy: 0.525000 Validation Accuracy: 0.595600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.1695 Training Accuracy: 0.625000 Validation Accuracy: 0.591600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.9612 Training Accuracy: 0.600000 Validation Accuracy: 0.585600\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.8075 Training Accuracy: 0.750000 Validation Accuracy: 0.586400\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     1.0508 Training Accuracy: 0.625000 Validation Accuracy: 0.593000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     1.1370 Training Accuracy: 0.525000 Validation Accuracy: 0.594400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.1560 Training Accuracy: 0.650000 Validation Accuracy: 0.594000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.9155 Training Accuracy: 0.650000 Validation Accuracy: 0.591600\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.7566 Training Accuracy: 0.775000 Validation Accuracy: 0.594400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     1.0893 Training Accuracy: 0.650000 Validation Accuracy: 0.595200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     1.1314 Training Accuracy: 0.625000 Validation Accuracy: 0.593000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.1844 Training Accuracy: 0.650000 Validation Accuracy: 0.595200\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.8906 Training Accuracy: 0.725000 Validation Accuracy: 0.590400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.7916 Training Accuracy: 0.775000 Validation Accuracy: 0.589800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     1.0840 Training Accuracy: 0.625000 Validation Accuracy: 0.593000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     1.1282 Training Accuracy: 0.625000 Validation Accuracy: 0.600200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.2144 Training Accuracy: 0.650000 Validation Accuracy: 0.590600\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.9297 Training Accuracy: 0.675000 Validation Accuracy: 0.595000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.7640 Training Accuracy: 0.800000 Validation Accuracy: 0.591000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     1.0599 Training Accuracy: 0.600000 Validation Accuracy: 0.592200\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     1.1111 Training Accuracy: 0.625000 Validation Accuracy: 0.600200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.1690 Training Accuracy: 0.600000 Validation Accuracy: 0.592200\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.9109 Training Accuracy: 0.725000 Validation Accuracy: 0.582000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.7925 Training Accuracy: 0.750000 Validation Accuracy: 0.592800\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     1.0684 Training Accuracy: 0.600000 Validation Accuracy: 0.595800\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     1.1192 Training Accuracy: 0.600000 Validation Accuracy: 0.596600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.1878 Training Accuracy: 0.650000 Validation Accuracy: 0.591400\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.8680 Training Accuracy: 0.725000 Validation Accuracy: 0.594600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.7598 Training Accuracy: 0.800000 Validation Accuracy: 0.596600\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     1.1025 Training Accuracy: 0.625000 Validation Accuracy: 0.594400\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     1.1055 Training Accuracy: 0.600000 Validation Accuracy: 0.599200\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1800 Training Accuracy: 0.625000 Validation Accuracy: 0.595200\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.9233 Training Accuracy: 0.725000 Validation Accuracy: 0.591000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.7680 Training Accuracy: 0.800000 Validation Accuracy: 0.594600\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     1.0778 Training Accuracy: 0.600000 Validation Accuracy: 0.597800\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     1.0947 Training Accuracy: 0.600000 Validation Accuracy: 0.602400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.1415 Training Accuracy: 0.625000 Validation Accuracy: 0.594200\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.8689 Training Accuracy: 0.725000 Validation Accuracy: 0.594400\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.8082 Training Accuracy: 0.725000 Validation Accuracy: 0.593200\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     1.0459 Training Accuracy: 0.650000 Validation Accuracy: 0.595800\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     1.0683 Training Accuracy: 0.650000 Validation Accuracy: 0.601800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.1143 Training Accuracy: 0.650000 Validation Accuracy: 0.602200\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.8802 Training Accuracy: 0.650000 Validation Accuracy: 0.599400\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.7518 Training Accuracy: 0.850000 Validation Accuracy: 0.596000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     1.0468 Training Accuracy: 0.625000 Validation Accuracy: 0.597200\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     1.1213 Training Accuracy: 0.575000 Validation Accuracy: 0.600800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.1549 Training Accuracy: 0.625000 Validation Accuracy: 0.601200\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.9041 Training Accuracy: 0.650000 Validation Accuracy: 0.591200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.7238 Training Accuracy: 0.750000 Validation Accuracy: 0.594200\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     1.0478 Training Accuracy: 0.625000 Validation Accuracy: 0.600000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     1.1107 Training Accuracy: 0.575000 Validation Accuracy: 0.600200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.1460 Training Accuracy: 0.625000 Validation Accuracy: 0.598400\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.9343 Training Accuracy: 0.675000 Validation Accuracy: 0.600600\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.7607 Training Accuracy: 0.775000 Validation Accuracy: 0.596000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     1.0176 Training Accuracy: 0.600000 Validation Accuracy: 0.597200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     1.0783 Training Accuracy: 0.625000 Validation Accuracy: 0.604200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.1544 Training Accuracy: 0.625000 Validation Accuracy: 0.598800\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.9263 Training Accuracy: 0.725000 Validation Accuracy: 0.596600\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.7313 Training Accuracy: 0.775000 Validation Accuracy: 0.597400\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     1.0466 Training Accuracy: 0.625000 Validation Accuracy: 0.594400\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     1.0623 Training Accuracy: 0.625000 Validation Accuracy: 0.603600\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.1547 Training Accuracy: 0.625000 Validation Accuracy: 0.603800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.8933 Training Accuracy: 0.725000 Validation Accuracy: 0.603600\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.7424 Training Accuracy: 0.775000 Validation Accuracy: 0.595400\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     1.0067 Training Accuracy: 0.600000 Validation Accuracy: 0.597600\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     1.0382 Training Accuracy: 0.600000 Validation Accuracy: 0.600400\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.1662 Training Accuracy: 0.625000 Validation Accuracy: 0.600000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.9049 Training Accuracy: 0.725000 Validation Accuracy: 0.601200\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.7750 Training Accuracy: 0.725000 Validation Accuracy: 0.593000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     1.0609 Training Accuracy: 0.625000 Validation Accuracy: 0.604200\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     1.0754 Training Accuracy: 0.600000 Validation Accuracy: 0.604400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.0971 Training Accuracy: 0.650000 Validation Accuracy: 0.604200\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.9297 Training Accuracy: 0.600000 Validation Accuracy: 0.608000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.7467 Training Accuracy: 0.750000 Validation Accuracy: 0.600000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     1.0253 Training Accuracy: 0.675000 Validation Accuracy: 0.601600\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     1.0372 Training Accuracy: 0.600000 Validation Accuracy: 0.605600\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.0955 Training Accuracy: 0.675000 Validation Accuracy: 0.601400\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.9113 Training Accuracy: 0.700000 Validation Accuracy: 0.608600\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.7536 Training Accuracy: 0.700000 Validation Accuracy: 0.596800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     1.0181 Training Accuracy: 0.600000 Validation Accuracy: 0.602200\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     1.0617 Training Accuracy: 0.625000 Validation Accuracy: 0.605000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.1241 Training Accuracy: 0.650000 Validation Accuracy: 0.605000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.8894 Training Accuracy: 0.750000 Validation Accuracy: 0.601600\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.7098 Training Accuracy: 0.800000 Validation Accuracy: 0.599600\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     1.0073 Training Accuracy: 0.600000 Validation Accuracy: 0.608800\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     1.0855 Training Accuracy: 0.575000 Validation Accuracy: 0.601000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.1013 Training Accuracy: 0.625000 Validation Accuracy: 0.607000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.8634 Training Accuracy: 0.700000 Validation Accuracy: 0.611600\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.7126 Training Accuracy: 0.800000 Validation Accuracy: 0.603200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.9614 Training Accuracy: 0.650000 Validation Accuracy: 0.603200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     1.0605 Training Accuracy: 0.625000 Validation Accuracy: 0.603400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.1183 Training Accuracy: 0.675000 Validation Accuracy: 0.602800\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.8875 Training Accuracy: 0.650000 Validation Accuracy: 0.606400\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.7292 Training Accuracy: 0.800000 Validation Accuracy: 0.601400\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.9763 Training Accuracy: 0.625000 Validation Accuracy: 0.603800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     1.0554 Training Accuracy: 0.575000 Validation Accuracy: 0.608000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.1206 Training Accuracy: 0.650000 Validation Accuracy: 0.601400\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.9136 Training Accuracy: 0.700000 Validation Accuracy: 0.602400\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.7303 Training Accuracy: 0.800000 Validation Accuracy: 0.601400\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.9980 Training Accuracy: 0.600000 Validation Accuracy: 0.606400\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     1.0558 Training Accuracy: 0.625000 Validation Accuracy: 0.603200\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.0903 Training Accuracy: 0.625000 Validation Accuracy: 0.600000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.8567 Training Accuracy: 0.650000 Validation Accuracy: 0.602400\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.6906 Training Accuracy: 0.800000 Validation Accuracy: 0.603800\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     1.0176 Training Accuracy: 0.625000 Validation Accuracy: 0.605200\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     1.0448 Training Accuracy: 0.625000 Validation Accuracy: 0.605200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.1070 Training Accuracy: 0.650000 Validation Accuracy: 0.606000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.8285 Training Accuracy: 0.750000 Validation Accuracy: 0.604200\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.7171 Training Accuracy: 0.775000 Validation Accuracy: 0.606400\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.9632 Training Accuracy: 0.625000 Validation Accuracy: 0.605800\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     1.0459 Training Accuracy: 0.625000 Validation Accuracy: 0.609600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6073971518987342\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREJPr/Li8OHu89RdEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1386f03ec88>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
